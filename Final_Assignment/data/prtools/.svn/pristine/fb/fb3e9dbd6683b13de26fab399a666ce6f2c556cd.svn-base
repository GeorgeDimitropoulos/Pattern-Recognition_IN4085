%BAGGINGC Bootstrapping and aggregation of classifiers
% 
%    W = BAGGINGC (A,CLASSF,N,ACLASSF,T)
% 
% INPUT
%   A         Training dataset.
%   CLASSF    The base classifier (default: nmc)
%   N         Number of base classifiers to train (default: 100)
%   ACLASSF   Aggregating classifier (default: meanc), [] for no aggregation.
%   T         Tuning set on which ACLASSF is trained (default: [], meaning use A)
%
% OUTPUT
%    W        A combined classifier (if ACLASSF given) or a stacked
%             classifier (if ACLASSF []).
%
% DESCRIPTION
% Computation of a stabilised version of a classifier by bootstrapping and
% aggregation ('bagging'). In total N bootstrapped versions of the dataset A
% are generated and used for training of the untrained classifier CLASSF.
% Aggregation is done using the combining classifier specified in CCLASSF.
% If ACLASSF is a trainable classifier it is trained by the tuning dataset
% T, if given; else A is used for training. The default aggregating classifier
% ACLASSF is MEANC. Default base classifier CLASSF is NMC.
%
% In multi-class problems another way of combining might be of interest:
% W = A*(BAGGINGC*QDC([],[],1e-6)).
%
% REFERENCE
% L.Beiman, Bagging Predictors, Machine Learning, 1996.
%
% SEE ALSO (<a href="http://37steps.com/prtools">PRTools Guide</a>)
% DATASETS, MAPPINGS, NMC, MEANC

% Copyright: R.P.W. Duin, duin@ph.tn.tudelft.nl
% Faculty of Applied Sciences, Delft University of Technology
% P.O. Box 5046, 2600 GA Delft, The Netherlands

% $Id: baggingc.m,v 1.3 2010/06/01 08:47:05 duin Exp $

function w = baggingc(varargin)


  mapname = 'Bagging';
  argin = setdefaults(varargin,[],nmc,100,meanc,[]);
  
  if mapping_task(argin,'definition')
    
    w = define_mapping(argin,'untrained',mapname);
    
  else

    [a,clasf,n,rule,t] = deal(argin{:});
    iscomdset(a,t); % test compatibility training and tuning set

    % Concatenate N classifiers on bootstrap samples (100%) taken
    % from the training set.

    w = [];
    s = sprintf('Generation of %i base classifiers: ',n);
    prwaitbar(prtime,s);
    starttime = clock;
    runtime = 0;
    for i = 1:n
      prwaitbar(prtime,runtime,[s num2str(i)]);
      if runtime > prtime
        prwarning(2,'Generation of base classifiers stopped by PRTIME at %i classifiers',i-1);
        break;
      end
      w = [w gendat(a)*clasf]; 
      runtime = etime(clock,starttime);
    end
    prwaitbar(0);

    % If no aggregating classifier is given, just return the N classifiers...

    if (~isempty(rule))

      % ... otherwise, train the aggregating classifier on the train or
      % tuning set.

      if (isempty(t))
        w = traincc(a,w,rule);
      else
        w = traincc(t,w,rule);
      end
    end

    w = setcost(w,a);
    
  end
	
return
